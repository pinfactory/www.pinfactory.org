A key problem with building an open ecosystem for AI is the cost of benchmarking. Even when it is possible to train a model at low cost, and to develop AI-based agents that show promise in performing tasks, the challenge in evaluating a new model or agent is one of resources. Accurately measuring an agent's "competence" often depends on either hiring large numbers of human reviewers, or paying for access to a larger system or service.

One promising approach is to use a prediction market as a benchmark for AI agents. Prediction markets have several attractive qualities that make them appropriate as a benchmark. A market can function as a general-purpose metric of an agent's ability to reflect the real world. And a successful trader on a prediction market can profit from interactions with many others, making markets scale automatically. Markets are also a canonical tool for measuring and offsetting risks and uncertainty, which makes them ideal for evaluating agents that can be overconfident about the answers they provide.

Although AI traders have so far had limited success on the public prediction markets, there is more promise in a more confined market in which all relevant information is readily available to the agent. This talk will demonstrate the rapid evaluation of multiple AI agents on the Pinfactory market, which is an open-source prediction market designed to trade on the state of issues in an issue tracker (currently GitHub issues). Pinfactory allows multiple agents to connect to the market site and to explore the underlying software issues and code, and to trade with each other. Which commonly used agents are "learning" to understand and assist with improving software quality, and which are overconfident? With a prediction market as an AI agent benchmark, we can let the money talk.
